# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eZ5O_kBvy_1Bcyh9ivROYFgibfen8vRx
"""

import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# ==========================================
# 1๏ธโฃ ุชุซุจูุช ุงูููุชุจุงุช ูุงุณุชูุฑุงุฏูุง
# ==========================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, classification_report, roc_curve
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import joblib
import os
import shap

print("โ ุชู ุงุณุชูุฑุงุฏ ุงูููุชุจุงุช ุจูุฌุงุญ.")

# ==========================================
# 2๏ธโฃ ุชุญููู ุงูุจูุงูุงุช
# (ุชุฃูุฏ ุฃู ุงููููุงุช ููุฌูุฏุฉ ูู ูุฌูุฏ Dataset)
# ==========================================
# ูุงุฆูุฉ ุงููููุงุช ุงููุทููุจุฉ ูุน ุงููุณุงุฑุงุช
file_paths = {
    'admissions': 'Dataset/hosp/admissions.csv',
    'patients': 'Dataset/hosp/patients.csv',
    'chartevents': 'Dataset/icu/chartevents.csv',
    'labevents': 'Dataset/hosp/labevents.csv',
    'transfers': 'Dataset/hosp/transfers.csv'
}

# ุงูุชุญูู ูู ูุฌูุฏ ุงููููุงุช
missing_files = [f for f, path in file_paths.items() if not os.path.exists(path)]
if missing_files:
    print(f"โ๏ธ ุชูุจูู: ุงููููุงุช ุงูุชุงููุฉ ุบูุฑ ููุฌูุฏุฉ: {missing_files}")
    print(f"ุงููููุงุช ุงููุทููุจุฉ: {file_paths}")
    print("ุชุฃูุฏ ูู ุฃู ุฌููุน ูููุงุช CSV ููุฌูุฏุฉ ูู ูุฌูุฏ Dataset/")
    exit(1)

print("โ ุฌุงุฑู ุชุญููู ุงููููุงุช...")
try:
    admissions = pd.read_csv(file_paths['admissions'])
    patients = pd.read_csv(file_paths['patients'])
    chartevents = pd.read_csv(file_paths['chartevents'])
    labevents = pd.read_csv(file_paths['labevents'])
    transfers = pd.read_csv(file_paths['transfers'])
    print("โ ุชู ุชุญููู ุงูุจูุงูุงุช.")
except Exception as e:
    print(f"โ ุฎุทุฃ ูู ุชุญููู ุงูุจูุงูุงุช: {e}")
    exit(1)

# ูุนุงูุฌุฉ ุงูุจูุงูุงุช ูู block ูููุตู
try:

    # ==========================================
    # 3๏ธโฃ ูุนุงูุฌุฉ ุงูุชูุงุฑูุฎ ูุชุญุฏูุฏ ุงูู Cohort
    # ==========================================
    # ุชุญููู ุงูุฃุนูุฏุฉ ุงูุฒูููุฉ
    for col in ["admittime", "dischtime"]:
        admissions[col] = pd.to_datetime(admissions[col])

    chartevents["charttime"] = pd.to_datetime(chartevents["charttime"])
    labevents["charttime"] = pd.to_datetime(labevents["charttime"])

    # ุชุญุฏูุฏ ููุทุฉ ุงูุชูุจุค (t0): ุจุนุฏ 24 ุณุงุนุฉ ูู ุงูุฏุฎูู
    admissions["t0"] = admissions["admittime"] + pd.Timedelta(hours=24)

    # ุงูููุชุฑุฉ: ูุณุชุจุนุฏ ุงููุฑุถู ุงููู ุฎุฑุฌูุง ูุจู ูุฑูุฑ 24 ุณุงุนุฉ
    cohort = admissions[admissions["dischtime"] > admissions["t0"]].copy()

    # ุชุญุฏูุฏ ุงููุฏู (Label): ุงูุฎุฑูุฌ ุฎูุงู 48 ุณุงุนุฉ ุจุนุฏ t0
    # ูุนูู ูุงูุฐุฉ ุงูุชููุน ุชุจุฏุฃ ูู t0 ูุชูุชูู ูู t0 + 48h
    cohort["label"] = (
        cohort["dischtime"] <= (cohort["t0"] + pd.Timedelta(hours=48))
    ).astype(int)

    print(f"ุนุฏุฏ ุงูุญุงูุงุช ูู ุงูุนููุฉ: {len(cohort)}")
    print(f"ูุณุจุฉ ุงูุฎุฑูุฌ ุงููุจูุฑ (Target): {cohort['label'].mean():.2%}")

    # ==========================================
    # 4๏ธโฃ ุฏูุฌ ุงูุจูุงูุงุช ุงูุฃุณุงุณูุฉ (Demographics)
    # ==========================================
    base = cohort.merge(
        patients[["subject_id", "gender", "anchor_age"]],
        on="subject_id",
        how="left"
    )

    # ==========================================
    # 5๏ธโฃ ููุฏุณุฉ ุงูุฎุตุงุฆุต (Feature Engineering)
    # ==========================================

    # --- ุฃ: ุงูุนูุงูุงุช ุงูุญูููุฉ (Vitals) ---
    VITAL_ITEMS = {
        "heart_rate": [220045],
        "sbp": [220179],
        "resp_rate": [220210],
        "spo2": [220277],
    }

    def extract_vitals(df, base_df):
        features = []
        # ูุฃุฎุฐ ููุท ุงูุจูุงูุงุช ุงูููุฌูุฏุฉ ูุจู t0
        # ุฏูุฌ ุฃููู ูุชุตููุฉ ุงูููุช
        temp = df[df["itemid"].isin([i for ids in VITAL_ITEMS.values() for i in ids])].copy()
        temp = temp.merge(base_df[["hadm_id", "t0"]], on="hadm_id")
        temp = temp[temp["charttime"] <= temp["t0"]]

        for name, itemids in VITAL_ITEMS.items():
            subset = temp[temp["itemid"].isin(itemids)]
            if subset.empty: continue

            # ุญุณุงุจ ุงููุชูุณุทุ ุงููุงูุณุ ูุงููููููู
            agg = subset.groupby("hadm_id")["valuenum"].agg(
                mean="mean", min="min", max="max"
            )
            agg.columns = [f"{name}_{c}" for c in agg.columns]
            features.append(agg)

        return pd.concat(features, axis=1) if features else pd.DataFrame()

    print("ุฌุงุฑู ุงุณุชุฎุฑุงุฌ ุงูุนูุงูุงุช ุงูุญูููุฉ...")
    vital_features = extract_vitals(chartevents, base)

    # --- ุจ: ุงูุชุญุงููู ุงููุฎุจุฑูุฉ (Labs) ---
    LAB_ITEMS = {
        "creatinine": [50912],
        "wbc": [51300],
        "hemoglobin": [51222],
        "sodium": [50983],
    }

    def extract_labs(df, base_df):
        features = []
        temp = df[df["itemid"].isin([i for ids in LAB_ITEMS.values() for i in ids])].copy()
        temp = temp.merge(base_df[["hadm_id", "t0"]], on="hadm_id")
        temp = temp[temp["charttime"] <= temp["t0"]]

        for name, itemids in LAB_ITEMS.items():
            subset = temp[temp["itemid"].isin(itemids)]
            if subset.empty: continue

            # ูุฃุฎุฐ ุขุฎุฑ ูููุฉ ูุจู t0
            last_val = subset.sort_values("charttime").groupby("hadm_id")["valuenum"].last()
            last_val = last_val.to_frame(name=f"{name}_last")
            features.append(last_val)

        return pd.concat(features, axis=1) if features else pd.DataFrame()

    print("ุฌุงุฑู ุงุณุชุฎุฑุงุฌ ุงูุชุญุงููู...")
    lab_features = extract_labs(labevents, base)

    # ==========================================
    # 6๏ธโฃ ุชุฌููุฒ ุงูุจูุงูุงุช ููุชุฏุฑูุจ
    # ==========================================
    # ุฏูุฌ ูู ุดูุก
    data = base.merge(vital_features, on="hadm_id", how="left")
    data = data.merge(lab_features, on="hadm_id", how="left")

    # ูุนุงูุฌุฉ ุงููุชุบูุฑุงุช ุงููุฆููุฉ
    data["gender"] = data["gender"].map({"M": 1, "F": 0})
    data = pd.get_dummies(data, columns=["admission_type"], drop_first=True)

    # ุชุญุฏูุฏ X ู y
    drop_cols = ["subject_id", "hadm_id", "t0", "label", "admittime", "dischtime",
                 "deathtime", "edregtime", "edouttime", "hospital_expire_flag",
                 "admit_provider_id", "admission_location", "discharge_location",
                 "insurance", "language", "marital_status", "race"]

    # ุญุฐู ุงูุฃุนูุฏุฉ ุบูุฑ ุงููุณุชุฎุฏูุฉ ุงูููุฌูุฏุฉ ูุนููุงู ูู ุงูุฏุงุชุง
    cols_to_drop = [c for c in drop_cols if c in data.columns]
    X = data.drop(columns=cols_to_drop)
    y = data["label"]

    # ูุนุงูุฌุฉ ุงูููู ุงูููููุฏุฉ (ููู ุฌุฏุงู ููุจูุงูุงุช ุงูุทุจูุฉ)
    # 1. ุฅุถุงูุฉ ุนููุฏ ูุคุดุฑ (Flag) ููููู ุงูููููุฏุฉ
    numeric_cols = X.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if X[col].isnull().any():
            X[f"{col}_missing"] = X[col].isnull().astype(int)

    # 2. ุชุนุจุฆุฉ ุงูููู ุงูููููุฏุฉ ุจุงููุณูุท (Median)
    X = X.fillna(X.median())

    # ==========================================
    # 7๏ธโฃ ุจูุงุก ุงููููุฐุฌ ูุงูุชุฏุฑูุจ
    # ==========================================
    # ุชูุณูู ุงูุจูุงูุงุช (Stratified ูุถูุงู ุชูุงุฒู ุงูุชูุฒูุน)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Pipeline: ุชูุญูุฏ ุงูููุงููุณ (Scaling) + ุงููููุฐุฌ
    # ุงุณุชุฎุฏููุง class_weight='balanced' ููุชุนุงูู ูุน ุนุฏู ุชูุงุฒู ุงูุจูุงูุงุช
    pipeline = Pipeline([
        ("scaler", StandardScaler()),
        ("model", LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42))
    ])

    print("ุฌุงุฑู ุชุฏุฑูุจ ุงููููุฐุฌ...")
    pipeline.fit(X_train, y_train)

    # ==========================================
    # 8๏ธโฃ ุงูุชูููู ูุงููุชุงุฆุฌ
    # ==========================================
    y_prob = pipeline.predict_proba(X_test)[:, 1]
    y_pred = pipeline.predict(X_test)

    auc = roc_auc_score(y_test, y_prob)
    print("\n" + "="*30)
    print(f"๐ ุงููุชุงุฆุฌ ุงูููุงุฆูุฉ (Test Set):")
    print(f"AUROC Score: {auc:.4f}")
    print("="*30)
    print("\nุชูุฑูุฑ ุงูุชุตููู ุงูููุตู:")
    print(classification_report(y_test, y_pred))

    # ุฑุณู ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f"Logistic Regression (AUC = {auc:.2f})", color='darkblue')
    plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve - Discharge Prediction")
    plt.legend()
    plt.grid(True)
    plt.show()

    # ==========================================
    # ๐ ุดุฑุญ ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู SHAP
    # ==========================================
    print("\n" + "="*50)
    print("๐ ุญุณุงุจ SHAP Values ููุชูุณูุฑ ุงูุฏููู ูููููุฐุฌ...")
    print("="*50)

    # ุงูุญุตูู ุนูู ุงููููุฐุฌ ูุงูู Scaler ูู Pipeline
    model = pipeline.named_steps["model"]
    scaler = pipeline.named_steps["scaler"]
    
    # ุชุญููู ุจูุงูุงุช ุงูุงุฎุชุจุงุฑ ุจุงุณุชุฎุฏุงู ููุณ ุงูู Scaler
    X_test_scaled = scaler.transform(X_test)
    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)
    
    # ุฅูุดุงุก SHAP explainer ุจุงุณุชุฎุฏุงู ูููุฐุฌ ูุฏุฑุจ
    # ุงุณุชุฎุฏู sample ูู ุจูุงูุงุช ุงูุชุฏุฑูุจ ูู background
    background_sample = X_test_scaled_df.sample(min(100, len(X_test_scaled_df)), random_state=42)
    explainer = shap.KernelExplainer(model.predict, background_sample)
    
    # ุญุณุงุจ SHAP values ููุจูุงูุงุช ุงูุงุฎุชุจุงุฑ
    shap_values = explainer.shap_values(X_test_scaled_df)
    
    # ุญูุธ SHAP values ููุท (ูุง ูุญุงูู ุญูุธ ุงูู explainer ูุฃููุง ุชุญุชูู ุนูู ุฏูุงู ูุง ูููู pickle-ing)
    shap_data = {
        'shap_values': shap_values,
        'X_test_scaled': X_test_scaled_df.values,
        'X_test_scaled_columns': X_test_scaled_df.columns.tolist(),
        'feature_names': X.columns.tolist(),
        'expected_value': explainer.expected_value,
        'background_sample': background_sample.values,
        'background_columns': background_sample.columns.tolist()
    }
    joblib.dump(shap_data, "shap_values_data.pkl")
    print("โ ุชู ุญุณุงุจ ูุญูุธ SHAP values ุจูุฌุงุญ")

    # ==========================================
    # ๐ ุฑุณู SHAP Summary Plot (Bar)
    # ==========================================
    print("\n๐ ุฑุณู SHAP Summary Plot (Feature Importance)...")
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_test_scaled_df, plot_type="bar", show=False)
    plt.title("SHAP Feature Importance - Discharge Prediction", fontsize=14, pad=20)
    plt.tight_layout()
    plt.show()

    # ==========================================
    # ๐ ุฑุณู SHAP Summary Plot (Beeswarm)
    # ==========================================
    print("\n๐ ุฑุณู SHAP Beeswarm Plot...")
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X_test_scaled_df, show=False)
    plt.title("SHAP Summary Plot - Impact on Discharge Prediction", fontsize=14, pad=20)
    plt.tight_layout()
    plt.show()

    # ==========================================
    # ๐ฏ ุนุฑุถ ุฃูู ุงูุนูุงูู ุงููุคุซุฑุฉ (Coefficients)
    # ==========================================
    print("\n" + "="*50)
    print("๐ ุฃูู ุงูุนูุงูู ุงููุคุซุฑุฉ ูู ุงูุชูุจุค (Coefficients):")
    print("="*50)
    coefs = pd.DataFrame({
        "Feature": X.columns,
        "Coefficient": model.coef_[0]
    }).sort_values(by="Coefficient", key=abs, ascending=False)

    print(coefs.head(15))

    # ==========================================
    # 9๏ธโฃ ุญูุธ ุงููููุฐุฌ ูุงูุจูุงูุงุช ุงููุฑุชุจุทุฉ
    # ==========================================
    joblib.dump(pipeline, "discharge_prediction_model.pkl")
    print("\nโ ุชู ุญูุธ ุงููููุฐุฌ ุจุงุณู 'discharge_prediction_model.pkl'")
    print("โ ุชู ุญูุธ SHAP data ุจุงุณู 'shap_values_data.pkl'")
    print("\n๐ ุงููููุฐุฌ ุฌุงูุฒ ููุงุณุชุฎุฏุงู ูู ููุญุฉ ุงูุชุญูู (app.py)")

except Exception as e:
    print(f"\nโ ุญุฏุซ ุฎุทุฃ ุฃุซูุงุก ูุนุงูุฌุฉ ุงูุจูุงูุงุช: {e}")
    print("ุชุฃูุฏ ูู ุฃู ูููุงุช CSV ุตุญูุญุฉ ูุชุญุชูู ุนูู ุงูุฃุนูุฏุฉ ุงููุทููุจุฉ.")
    exit(1)